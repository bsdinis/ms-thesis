\fancychapter{Leveraging the \ac{RR} model in distributed storage}\label{chap:storage}
\cleardoublepage{}

% Story 1: bottom-up
%  1. Batching can improve device throughput

%  2. Waiting for batches introduces delays: latency suffers

%  3. We can be eager, reply once the data is in an in-memory
%  buffer

%  4. This introduces a fault-tolerance problem: if the device
%  crashes before the buffer is flushed, the data is lost

%  5. In replicated systems, we can protect against data loss by
%  replying eagerly in some replicas and waiting for data to be
%  in disk in others. If done strategically, it ensures that
%  enough replicas have committed and enough replicas replied
%  fast

%  6. The replicated system needs to tolerate forgetful nodes:
%  if there is a crash and a node loses its soft state it needs
%  to be able to signal that it may be offering stale state back.

%  7. This is exactly the model offered by the restart-rollback
%  model, originally conceived in the context of Trusted
%  Execution Environments with persistent storage.

%  8. This combination is far from trivial. It requires careful
%  adaptations to the storage layer to support different modes
%  of operation and realizing the throughput benefits through
%  batching. At the protocol level, this asymmetry of operation
%  between replicas (with some replying eagerly while others
%  flushing to disk) introduces the challenge of scheduling these
%  sets. This scheduling is crucial to maximize performance, and offers
%  a rich design space.

%  9. Our construction also introduces deployment challenges:
%  the parametrization of the different types of faults the
%  system can suffer is not obvious. We explain how to reason
%  about the reliability and availability of the system to, in
%  conjunction with empirical evidence, derive an adequate
%  deployment for the system requirements.

%  10. Contributions:
%      - Application of a recent fault model to achieve batching
%      throughput benefits without sacrificing safety
%      - The design of a novel storage layer
%      - An exploration of flush scheduling and its impact on
%      throughput and latency
%      - Evidence based parametrization of the resulting system, informed by the
%      desired reliability and availability
%      - PoC implementations and evaluation

% @bsd: problems with the story: I think this misses the whole
% "arc" of persistence in distributed systems, which I think is
% an important contribution

% 1, 2, 3 and 4: the whys, whats and hows of batching
Batching multiple small write operations to a block-oriented storage device into a
single large one is a known mechanism to increase its throughput.
To achieve this batching, one has to wait for an adequate amount
of contiguous data to write, which introduces a delay in the
operation. Alternatively, once the data to write is present in
an in-memory buffer (soft state) the system could return to the
client. This opens the possibility of data loss, if the system
crashes before effectively synchronizing this data to the persistent
storage device.

% 5: replication should be able to partially mask this
%
% note: we can introduce the difference between persistent replicated
% storage systems and volatile replicated storage systems
This problem is propagated as is to current persistent replicated storage
systems: the throughput of the system is limited by the
throughput of the storage devices of the replicas. To avoid this
performance penalty, some replicated systems consider data to be persisted if it is
present in volatile memory at a sufficiently large subset of replicas~\cite{pbft}
(i.e.\ persistence through replication). Such an approach is not
consensual, with other authors insisting that data must be
present in stable storage to be considered persisted~\cite{}.

% Question whether this needs to be a dichotomy
We argue that these two properties --- performance and
durability --- do not need to be mutually exclusive. Instead, we
aim to combine both into a system that offers the throughput from
the solutions which persist in the background with the high
durability from systems which persist before replying to
requests.

% We present a new replication strategy
To this end, we present a novel replication strategy based on
carefully and strategically mixing both approaches, leveraging
\ac{RR} quorum systems. Instead of issuing the write
requests blindly to all replicas, as is costumary, we can
strategically signal to certain replicas to synchronize to
persistent storage before replying while allowing others to
acknowledge the write eagerly, thus mixing both approaches. This
key insight allows us to take advantage of batching, improving the overall
performance of the system, without relinquishing fault tolerance.
The \ac{RR} fault model is needed in case replicas crash before
synchronizing their soft state to stable storage.

% 8, 9: challenges
This approach opens a series of challenges. The storage layer
requires careful adaptations to support multiple modes of
operation the replication protocol needs tap into to orchestrate
the overall system, while offering efficient batching, even on
non-sequential writes. At the protocol level, the asymmetry of
operation between replicas introduces the problem of \emph{sync
scheduling}: choosing which replicas synchronize to persistent storage
and which can reply eagerly. This schedule is paramount to
extracting the maximum performance of the system, and offers a
rich design space. Parametrizing a deployment of such a system is
also not obvious. There is a tradeoff to be explored between the
total number of replicas, the number of replicas which can be
allowed to suffer a rollback (and, by extension, the performance
of the system \bsd{is this immediately clear?}) and the desired
availability and reliability of the system. We present a
principled method for this parametrization, where a system
administrator can tune the number of replicas based on how prone
to faults they are and the desired availability and durability
levels.

In this chapter, we will present the \acf{R2-S2}, a replicated
\ac{KVS} based on LevelDB~\cite{leveldb}, a single node \ac{KVS}
based on \ac{LSM}~\cite{lsm}, where we have implemented several modes
of replication to compare between the different approaches. The
design of the \ac{R2-S2} system is presented in
Section~\ref{sec:r2s2design} and the guidelines for system
parametrization discussed in
Sections~\ref{sec:r2s2parametrization}.
Section~\ref{sec:r2s2implementation} describes the implementation
of the PoC and a preliminary evaluation is presented in
Section~\ref{sec:r2s2evaluation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\ac{R2-S2} Design}\label{sec:r2s2design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section presents the design of \ac{R2-S2}
based on the principle of \emph{asymmetric synchronization}. However,
before discussing our architecture in detail, it is worthwhile to
present the three approaches that can be used to synchronize
replica state to stable storage.

\paragraph{Sync immediately.} When receiving an object to write, a
replica immediately writes and flushes the object to stable
storage, issuing the reply afterwards. This prevents batching,
which favours \emph{durability} and \emph{latency} in detriment
of \emph{throughput}.

\paragraph{Batch and wait.} When receiving an object to write, a
replica places the object in a batch and waits for this batch to
be flushed to stable storage. Afterwards, the reply can be
issued. Since there is batching and objects are being persisted
before replying, this approach favours \emph{durability} and
\emph{throughput} at the cost of \emph{latency} (which can be
very large if the batch takes a long time to be filled due to a
lack of writes).

\paragraph{Move fast and break things.} In this approach,
replicas place the object in the batch (i.e.:\ volatile memory)
and reply immediately. This achieves the best \emph{latency} and
\emph{throughput}, sacrificing \emph{durability} since replicas
can suffer a rollback if they crash and restart.

We now present the principles and goals for the design of
\ac{R2-S2}:
\begin{enumerate}
    \item \textbf{Strong Persistence}: data loss should never
        ocurr, even in the presence of arbitrary node crashes;

    \item \textbf{Performance}: the architecture should yield
        performance gains compared to
        Approaches A and B\@;
\if 0
    \item \textbf{Generality}: the architecture should be as
        generic as possible. In particular, it should be
        applicable to a variety of replication protocols.
\fi
\end{enumerate}

Note that none of our baselines satisfy all requirements:
approaches A and B achieve
\emph{strong persistence} but not the performance requirement.
Approach C does meet the performance
requirement, but does not satisfy \emph{strong persistence}.

In the remainder of this section, we will describe
\emph{asymmetric synchronization}, the key technique which realizes the
goals prescribed above, and how the architecture needs to
acommodate this paradigm to achieve the best possible
performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Asymmetric Synchronization}\label{ssec:asymmetric_synchronization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Asymmetric synchronization is the key to extracting performance
in \ac{R2-S2}. The core idea is to partition the replica set
into two disjoint subsets: the \emph{volatile set}
$\mathbbm{R}$, composed of the replicas which eagerly reply to the
request before synchronizing to persistent storage (and thus can
suffer a rollback on restart); and the sync set $\mathbb{F}$, the
replicas which wait until the data is persisted before replying.
When issuing a write request, each replica is told in which set
they belong to, and act accordingly. An important subset of
$\mathbb{F}$ is its intersection with the write quorum
$\mathbb{W}_Q$. This subset is comprised by the replicas that are
critical to the performance of the write operation (in
particular, its latency) and is referred as the \emph{critical
sync set} from now on.

\paragraph{Achieving strong persistence.} To achieve strong
persistence, it is sufficient that the critical sync set is
never null. This implies that there is always at least one
replica in every write quorum which persisted the value, making
it recoverable. This is guaranteed by the \ac{RR} quorum
system, by setting the maximum size of $\mathbbm{R}$ to $M_R$,
since the volatile replicas are the ones which can suffer the rollback on
restart.

The key to improving throughput is scheduling. Intuitively, it is
important to distribute the burden of syncing to disk among the
replica set, allowing all the replicas to batch eventually.

Regretablly, although this approach does yield performance
benefits compared to approaches A and B, it is impossible to
match the performance of C\@: this would require removing any
persistent accesses from the critical path, making $\mathbb{W}_Q
\cap \mathbb{F} = \varnothing$, violating strong persistence.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Storage Layer}\label{ssec:storage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The storage layer plays a crucial role in realizing the
performance potential of the system. It needs to efficiently support
three different types of write operations, one for each of the
different baseline approaches. It will also need to support read operations
efficiently. Figure~\ref{fig:storage_layer} shows a diagram of
the storage layer and its data-structures.

An \ac{LSM} is employed as the core data structure of the storage
layer, enabling the sequential writes required for batching as
well as providing reasonable read performance. There exists a
log, used to write values that need to be synced immediately, and
an in-memory batch for values that should be batched. To further speed
up reads there is an optional in-memory object cache.

There is an additional in-memory cache, which only stores the
object versions. This is useful because versions are a staple of
most replication protocols and several of them require reading
the current version before writing a new one. Since versions are
constant-sized and small, the capacity of the version cache can
be significantly larger than that of the object cache. This cache
is required to synchronize concurrent writes of different
versions to the same key. When the node is writing a value, it required to
check the version cache, populating it if required, where an
atomic update is executed on the version to ensure that an old
version does not override a newer version it is racing with.

Satisfying read requests is trivial: the appropriate cache is
consulted (depending on whether it is an object or a version
read), falling back on the \ac{LSM} and the log in the event of a miss.

Handling batched writes is also quite simple: the
value is placed in the batch and eventually a background thread
will flush the batch into the \ac{LSM}.

If the object should be immediately synced, it is placed into the
persistent log, and incorporated into the \ac{LSM} at a later
time.

\begin{figure}[t]
    \centering
    \includegraphics[width=.75\linewidth]{img/storage_layer}
    \caption{Diagram of the Storage Layer}\label{fig:storage_layer}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scheduling}\label{ssec:schedule}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


As we have covered, a good schedule is crucial to realize the
performance potential of the architecture. A \emph{schedule} is a
sequence $S$, where $S_i^j$ is the mode of operation (batch and
reply, batch and wait or sync to log) to be applied by replica
$i$ for the $j^\text{th}$ write operation. The
schedule is said to be \emph{valid} if the volatile set (i.e.:
the number of batch and reply modes of operation) does not exceed
$M_R$, for every $S^j$. Thus, any valid schedule guarantees strong persistency.

There are several approaches to scheduling. Which is preferable
is non-obvious beforehand: the schedule will be sensitive to the
particularities of the replication protocol, the deployment and
the operation load. In the preliminary evaluation in
\S~\ref{sec:r2s2evaluation} we present an
evaluation of some of the scheduling policies described
here.

At a high level, there are two types of schedules: \emph{static}
schedules, which can pre-determined ahead of time, and as such
independent of the load the system is being subjected to; and
\emph{dynamic} schedules, which are computed on the fly and as
such can be \emph{reactive} to the load and current status of the
system.
\bsd{maybe remove: only did static schedules}


\paragraph{Constant Schedule.} A constant schedule is one where
all write operations use the same modes of operation for each
replica. In other words, all $S^j$s are equal. This schedule is,
at face value, a poor choice, since it prevents the replicas in the
critical set from ever batching. However, if the replica set is
very heterogeneous, this might be used to provide a tailored
approach to the hardware characteristics.

\paragraph{Random Schedule.} A schedule where replicas are
randomly allocated to the sync and volatile sets (based on the
sizes they need to have for the schedule to be valid).

\paragraph{Round Robin Schedule.} A schedule where the sync set
is shifted at each round. For instance, in a system with $N = 5,
M_R = 2$, replicas $0, 1, 2$ are the sync set at step $j$, and
at step $j + 1$ replicas $3, 4, 0$ become the sync set, and so
on.

Assuming a good random number generator, both the random and the
round robin schedules should evenly distribute the load among the
replicas. However, the random scheduler may offer less
predictable performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parametrizing the System}\label{sec:r2s2parametrization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To parametrize our system, we effectively need to choose
appropriate values for $M_R$ and $F$. We employ a data-driven
approach to this problem. Relying on real-world information about
the availability and reliability of components, we can predict
the reliability and availability of the system as a function of
$M_R$ and $F$. Choosing the adequate values becomes an exercise of
setting the desired reliability/availability and solving for $M_R$
and $F$.

The availability of the system (probability that it is available
at a particular point in time) is impacted by the availability of
the individual replicas. Let $p_c$ be the symmetric of the
availability of a replica, ie: the probability that a replica is
\emph{crashed} at a particular point in time.

The reliability (probability that the system loses data) of the
system is dependent on the reliability of the underlying storage
devices and the availability of the replicas in the eager set.
Let $p_f$ be the probability that a storage device fails
(irrecoverably).

These metrics will also be deployment sensitive. Let us consider
that the replicas are geo-replicated. In this scenario, replica
crashes are not correlated, and as such the probability that an
eager set crashes at the same time is $p_c^r$. Now let us imagine
that all replicas are within a single datacenter. Here if a
replica crashes it is highly likely that the others also crashed
(eg: the crash was caused by a power outage), meaning that the
probability that the eager set crashes is still $p_c$.

Table~\ref{tab:parametrization} summarizes the availability and
reliability of the system in these two scenarios.

\begin{table}[ht]
    \centering
    \caption{Availability and reliability in a geo-replicated
    deployment versus a datacenter deployment. $R_Q$ and $W_Q$
    are the sizes of the quorums for a parametrization with $M_R$
    tolerated rollbacks and $F$ crash faults}\label{tab:parametrization}
    \begin{tabular}{|r||c|c|}
        \hline
        & \textbf{Geo-replicated} & \textbf{Datacenter} \\ \hline
        \textbf{Availability} & $1 - p_c^{\min(R_Q, W_Q)}$ & $1 - p_c$ \\ \hline
        \textbf{Reliability}  & $1 - p_c^{M_R} \cdot p_f^{W_Q - M_R}$ & $1 - p_c \cdot p_f^{W_Q - M_R}$ \\ \hline
    \end{tabular}\label{tab:parametrization}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}\label{sec:r2s2implementation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We implemented \ac{R2-S2} using LevelDB~\cite{leveldb} as the
underlying storage layer and our own replication layer on top of
it. This was extremely helpful since LevelDB already implements
the \ac{LSM}, the persistent log and the optional object cache
out of the box, and provides for two out of the three modes of
operation: batch and reply and sync and reply. For the
replication layer, we chose to implement the \ac{ABD} distributed
register protocol, since it matches perfectly with the \ac{KVS}
interface.

The replication layer was implemented in Rust with the request transport
being handled by gRPC~\cite{grpc}. The \ac{R2-S2} client library
provides a shim that interacts with the replication layer that
coordinates the \ac{ABD} operations. The \ac{R2-S2} server
receives requests via gRPC and manages the timestamp cache and
its embedded LevelDB instance. Figure~\ref{fig:r2s2arch}
summarizes the architecture of the prototype.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/r2s2_arch}
    \caption{Architecture of the \ac{R2-S2} prototype}
\end{figure}

The client library was implemented in approximately 2KLoC of
Rust. The server was implemented in 400LoC. The gRPC protocol
definition is comprised of 135 LoC. All the remainder driver code
for benchmarks consists of 2KLoC of Rust.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}\label{sec:r2s2evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bsd{TODO}
